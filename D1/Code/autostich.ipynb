{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as num\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySIFTInstance = cv2.SIFT_create()\n",
    "bf = cv2.BFMatcher()\n",
    "\n",
    "class SIFTImage:\n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "\n",
    "        keypoints, descriptors = mySIFTInstance.detectAndCompute(image, None)\n",
    "        self.keypoints = keypoints\n",
    "        self.descriptors = descriptors\n",
    "\n",
    "        self.id = uuid.uuid1()\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Overrides the default implementation\"\"\"\n",
    "        if isinstance(other, SIFTImage):\n",
    "            return self.id == other.id\n",
    "        return False\n",
    "\n",
    "\n",
    "class MatchedImagePair:\n",
    "    def __init__(self, siftImage1, siftImage2):\n",
    "        self.siftImage1 = siftImage1\n",
    "        self.siftImage2 = siftImage2\n",
    "\n",
    "        self.matches = bf.knnMatch(siftImage1.descriptors, siftImage2.descriptors, k=2)\n",
    "\n",
    "def matchedPairContains(matchedPairs, image1, image2):\n",
    "    for matchedPair in matchedPairs:\n",
    "        if (matchedPair.siftImage1 == image1 and matchedPair.siftImage2 == image2) or (matchedPair.siftImage1 == image2 and matchedPair.siftImage2 == image1):\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "# 1. Match Features\n",
    "\n",
    "#The first step is to extract features from the images, and automatically establish feature correspondences between image pairs.\n",
    "\n",
    "# The starting point is an unordered set of N images. The objective of this step is to extract features from each image, \n",
    "# and establish which (if any) of the N-1 other images contain a sufficient number of good matches so as to indicate that \n",
    "# the two images partially overlap. You can use any feature that you prefer (e.g. ORB, SIFT, SURF) or some combination thereof.\n",
    "# You will need to develop a match metric that can be used to discriminate between good and bad image matches. For example, \n",
    "# this match metric could take the number of matching features between images into account, as well as the quality of their \n",
    "# match. Keep in mind that each image will only overlap with a small number of other images in the set, so ideally this match \n",
    "# metric will score high for the images that do have overlap, and low otherwise.\n",
    "\n",
    "imagePath = os.path.expanduser('~/Autostich-Maly/D1/Images/StJames/*.jpg')  #CAN CHANGE DEPENDING ON TEST IMAGES\n",
    "\n",
    "images = [cv2.imread(file) for file in glob.glob(imagePath)]\n",
    "\n",
    "siftImages = []\n",
    "\n",
    "for image in images:\n",
    "    (keypoints, descriptors) = mySIFTInstance.detectAndCompute(image, None)\n",
    "\n",
    "    siftImage = SIFTImage(image)\n",
    "\n",
    "    siftImages.append(siftImage)\n",
    "\n",
    "#This is test code to ensure keypoints are added correctly\n",
    "# testSiftImage = siftImages[0].image\n",
    "# testSiftImageKeyPoint = siftImages[0].keypoints\n",
    "\n",
    "# imageHeight, imageWidth, imageChannels = testSiftImage.shape\n",
    "# blankImage = num.zeros((imageHeight, imageWidth, imageChannels), num.uint8)\n",
    "# testSiftImageOutput = cv2.drawKeypoints(testSiftImage, testSiftImageKeyPoint, blankImage)\n",
    "# plt.imshow(testSiftImageOutput)\n",
    "\n",
    "matchedPairs = []\n",
    "\n",
    "count = 0 \n",
    "\n",
    "for testSiftImage in siftImages:\n",
    "    for siftImage in siftImages:\n",
    "        if (siftImage == testSiftImage): \n",
    "            continue\n",
    "\n",
    "        if (matchedPairContains(matchedPairs, siftImage, testSiftImage)):\n",
    "            continue\n",
    "\n",
    "        count += 1\n",
    "        matchedPairs.append(MatchedImagePair(siftImage, testSiftImage))\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this stores an array of comparisons that have aalready been performed \n",
    "#in order to prevent 'b' being tested with 'a', if 'a' had already been tested with 'b'\n",
    "testedImagePairs = []\n",
    "\n",
    "\n",
    "\n",
    "# 2. Estimate Transformation\n",
    "\n",
    "#The second step is to use these correspondences to estimate transformations between each image, and establish the most likely transformations between the image pairs.\n",
    "\n",
    "# For those image pairs that have a high enough match metric score from Step 1, calculate the transformation between them.\n",
    "\n",
    "\n",
    "\n",
    "# 3. Merge Images\n",
    "\n",
    "# The third step is to apply these transformations to compose a single merged composite image from all images.\n",
    "\n",
    "# Apply the transformations calculated in Step 2, and create and store a single merged image from the N images in the set. \n",
    "# Apply both geometric and radiometric transformations, so that the resulting merged image appears relatively seamless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "447b9f6d9193ed353982208596bd9bf11ade324ab35f2e41c29ceead09cfa85e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
